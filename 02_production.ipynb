{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10529280,"sourceType":"datasetVersion","datasetId":6515836}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom pathlib import Path\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom PIL import Image\nfrom fastai.vision.all import *","metadata":{"_uuid":"4a334d55-468f-4bd0-acf2-7c6bbb778a1d","_cell_guid":"93725786-c71e-4643-afcf-355b54cccb59","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Input data files are available in the read-only \"../input/\" directory\n\nFor example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory","metadata":{"_uuid":"3edda29f-daf3-44da-8f37-f4fad65ac4f5","_cell_guid":"960db07e-9717-44b8-97e6-0882de566336","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"input_dir = Path('/kaggle/input')\nfor filepath in input_dir.rglob('*'):\n    print(filepath)","metadata":{"_uuid":"26d40103-b1c5-4569-8bc2-35daa93b77c2","_cell_guid":"f73f62af-3c0a-4299-83d1-47cd24e2e587","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n* You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"01253e48-beb2-4c8d-a145-ee9601a3465a","_cell_guid":"c3930280-04f1-4594-bb23-dd1806d10e88","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Get Search Key\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nkey = user_secrets.get_secret(\"AZURE_SEARCH_KEY\")\nif not key:\n    raise ValueError(\"Azure Search Key not found in secrets\")\n","metadata":{"_uuid":"087c4854-a398-416a-b9ed-b4a0194dd906","_cell_guid":"bc7f34b7-4aa1-492c-8b52-2ca0dad95a13","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install fastbook > /dev/null\n\n!pip install ipywidgets==7.7.5  > /dev/null","metadata":{"_uuid":"62971c08-244f-4f62-80fd-e45b721841d1","_cell_guid":"a7586e13-5f3f-4eb4-9099-6ddea974ffc4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Fetching Image URLs from Bing\n\n- **Objective**: Retrieve a collection of image URLs for the query `'grizzly bear'` from Bing.\n- **Result**: The variable `ims` will store the collection of URLs.\n- **fastbook**: Part of the FastAI ecosystem, it provides utilities, functions, and code designed to assist in learning and implementing machine learning concepts.","metadata":{"_uuid":"b1bd4ddc-8f8b-47df-b890-bfbfcbbefc11","_cell_guid":"b33fd5f5-2d7a-4a5d-b847-a6196e9513f1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from fastbook import search_images_bing\nsearch_images_bing\ntry:\n    results = search_images_bing(key, 'grizzly bear')\n    ims = results.attrgot('contentUrl')\nexcept Exception as e:\n    print(f\"Error fetching search results: {e}\")\n    ims = []\nlen(ims)","metadata":{"_uuid":"3e754672-55bc-4b14-8554-0ba20f556c2c","_cell_guid":"8a2efe63-91b7-44b5-997a-acdd419c90fb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Downloading and displaying an image\n\n## Download one of the images from the URL collection as `images/grizzly.jpg`\n- **fastdownload**: A lightweight Python library designed to simplify the downloading and caching of datasets, models, and other resources.\n- **download_url**: Downloads the provided URL to the specified destination and shows the progress if desired (default is `True`).\n\n## Open the image using PIL (Python Image Library)\n- **python image library** is used for opening, manipulating, and saving image files\n- **Image.open**: opens the provided image path\n- **to_thumb(128,128)**: displays the image as a thumbnail","metadata":{"_uuid":"a67427a0-af60-4d23-9271-02feef1161b7","_cell_guid":"44d45d52-0191-4e1c-9da5-b32d1542224c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from fastdownload import download_url\ndest = 'images/grizzly.jpg'\ndownload_url(ims[1], dest)\n\nfrom PIL import Image\nim = Image.open(dest)\nim.to_thumb(128,128)","metadata":{"_uuid":"e78135f2-df34-4f49-8f23-0a4e69f5e09b","_cell_guid":"2b2f93ef-30fa-45b8-b9fb-40c453387a18","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Downloading Images for Each Bear Type\n\n**Iterating Over Bear Types**:\n  - The **Path** class is used for handling file system paths.\n      - *NOTE: the Path object is not creating the directory on the file system; it is just defining a representation of that path*\n  - Constructs a `Path` object for each bear type's directory (e.g., `bears/grizzly`).\n  - Ensures the directory and any necessary parent directories are created using `mkdir`.\n  \n**Using Bing Search**:\n  - Searches Bing for images of the current bear type using the `search_images_bing` function.\n  - Extracts URLs of the images from the search results.\n      - **results** -information about the found images, including their URLs.\n      - **urls=results.attrgot('contentUrl')** - creates a collection of urls from the 'contentUrl' property of each result\n\n**Downloading Images**:\n  - Downloads the images from the collected URLs into the appropriate directory (`dest`) using `download_images`.","metadata":{"_uuid":"0a332fb9-06ef-41f3-989a-dda29e105a1d","_cell_guid":"b078746f-82a0-44d7-870f-319464d3f3af","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from pathlib import Path\nfrom fastai.vision.all import download_images\n\nbear_types = 'grizzly','black','teddy'\n# a Path object representing the directory bears\npath = Path('bears')    \n\n# Iterates over each type of bear in the tuple bear_types (i.e., 'grizzly', 'black', 'teddy').\nfor o in bear_types:\n    dest = (path/o) \n    dest.mkdir(parents=True, exist_ok=True)  \n    results = search_images_bing(key, f'{o} bear') \n    download_images(dest, urls=results.attrgot('contentUrl'))","metadata":{"_uuid":"001e3421-9f0b-4b0b-8feb-7f00b1db7cf6","_cell_guid":"9bab5aa5-cb61-4a2d-b2de-ddf117d41c3c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from fastai.vision.all import DataBlock, ImageBlock, CategoryBlock, RandomSplitter, parent_label, get_image_files, Resize, ResizeMethod","metadata":{"_uuid":"5233e85d-5a69-4a2c-b6ca-e036d9c5b275","_cell_guid":"e1e6ddff-f963-4207-a479-1b0cc086f092","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Validating and Cleaning Up Image Files\n\nThis code is responsible for validating a collection of image files and removing any invalid or unreadable images. Here's a step-by-step explanation:\n\n### Step 1: Retrieve All Image Files\n- **Function**: `get_image_files(path)`\n  - Recursively retrieves all image files from the specified directory (`path`).\n  - Returns a list of file paths.\n- **Result**: Stores the list of image file paths in the variable `fns`.\n\n### Step 2: Verify Image Files\n\n- **Function**: `verify_images(fns)`\n  - Verifies whether each file in `fns` is a valid and readable image.\n  - Returns a collection of file paths (`failed`) for images that cannot be opened or are corrupted.\n\n### Step 3: Remove Invalid Images\n\n**map** applies a function to every element in the collection,\n\n- **Function**: failed.map(Path.unlink)`:\n  - Applies the `Path.unlink` method to each file path in the `failed` collection.\n\n- **Function**: Path.unlink()`:\n  - Deletes the file from the filesystem.\n  - Ensures that only valid image files remain in the dataset.","metadata":{"_uuid":"508f133a-678f-4cd5-a952-323fa267b5ea","_cell_guid":"d0a4df23-d4f2-4b5f-a805-a2a9bcee48b1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from fastai.vision.utils import get_image_files, verify_images\n\nfns = get_image_files(path)\nprint(f\"Number of images found: {len(fns)} in path {path}\")\n\nfailed = verify_images(fns) \nprint(f\"Number of images that can't be opened: {len(failed)}\")\n\nfailed.map(Path.unlink);","metadata":{"_uuid":"7b7b8b99-b886-435e-96d5-34373a462658","_cell_guid":"2e33599b-ecaf-4cce-aafc-2f682292086d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## DataBlock: Defining the Dataset Processing Pipeline\n\nThis `DataBlock` defines the steps for processing a dataset, and the specific `DataBlock` object here is named `bears`.\n\n### Key Components of the `bears` DataBlock\n\n#### 1. **`blocks`**\n   - Specifies the types of input (`x`) and target (`y`) data.\n   - In this case:\n     - `ImageBlock`: The input data consists of images.\n     - `CategoryBlock`: The target (label) is a category (e.g., the type of bear).\n\n#### 2. **`get_items`**\n   - Defines how to retrieve the input data.\n   - Uses `get_image_files`, a function that recursively retrieves all image files from a given path.\n\n#### 3. **`splitter`**\n   - Determines how to split the dataset into training and validation sets.\n   - Uses `RandomSplitter` with the following parameters:\n     - `valid_pct=0.2`: 20% of the data is assigned to the validation set.\n     - `seed=42`: Ensures the random split is reproducible (the same split will occur with each run).\n\n#### 4. **`get_y`**\n   - Defines how to extract the label (category) for each input.\n   - Uses `parent_label`, which assumes the label is the name of the parent directory containing the image file.\n\n#### 5. **`item_tfms`**\n   - Specifies transformations to apply to each individual item in the dataset.\n   - Applies `Resize(128)`, which resizes all images to 128x128 pixels to standardize input sizes for the model.","metadata":{"_uuid":"c021802f-b997-4a49-b84b-e9cba04aac84","_cell_guid":"a7df4e4b-af35-48e6-bce4-ca284e8d7b89","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"bears = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=Resize(128)\n)","metadata":{"_uuid":"8bd5815c-5f76-4756-87a6-4cd9e20454f5","_cell_guid":"dbb90b0e-f22a-4d3e-a26a-f67015409d97","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Understanding the `DataLoaders` Object\n\n- A **`DataLoaders`** object is a FastAI abstraction for managing and iterating over datasets during the training and validation process.\n- **Purpose**:\n  - Simplifies the process of loading data into batches.\n  - Wraps together one or more `DataLoader` objects:\n    - `dls.train`: For training data.\n    - `dls.valid`: For validation data.\n\n---\n\n### Creating a `DataLoaders` Object\n\n- **`bears`**: The `DataBlock` defined earlier, specifying how the dataset is structured and processed.\n- **`dls`**: The resulting `DataLoaders` object created by calling `.dataloaders(path)` on the `DataBlock`.\n  - `path`: The directory where the dataset is stored.\n \n---\n\n### Visualizing Validation Data\n\n- **`dls.valid.show_batch()`**:\n  - Displays a batch of validation data (images and labels) for inspection.\n\n#### Parameters:\n- **`max_n=4`**:\n  - Displays up to 4 samples from the validation set.\n- **`nrows=1`**:\n  - Arranges the images in a single row for easy viewing.","metadata":{"_uuid":"b6c56419-353b-4a5b-a017-83474c219dc3","_cell_guid":"f084a0be-37d3-42c4-a7df-dd75e2299f45","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"dls = bears.dataloaders(path)\ndls.valid.show_batch(max_n=4, nrows=1)","metadata":{"_uuid":"130c813c-82bc-4c01-8f05-a546b467920e","_cell_guid":"92a8ea8a-bdc2-4680-b546-cb707e8b875d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Explanation of `ResizeMethod.Squish`\n\n- **`ResizeMethod.Squish`**:\n  - This method resizes an image by squishing it into the specified dimensions without preserving the original aspect ratio.\n  - The image's width and height are adjusted independently to fit the target size, which can result in distortion if the aspect ratio of the original image differs from the target dimensions.\n\n#### Use Case:\n- Useful when maintaining the aspect ratio is not a priority, and you need a quick and simple way to standardize image dimensions.\n\n#### Example:\nIf an image of size `200x400` is resized to `128x128` using `ResizeMethod.Squish`, the output image will be exactly `128x128` but appear stretched or compressed.","metadata":{"_uuid":"3a13f251-705f-437c-82b8-92f8cf6ba9c8","_cell_guid":"f7d05802-d4fb-4d19-999c-b04d2f53a56b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"bears = bears.new(item_tfms=Resize(128, ResizeMethod.Squish))\ndls = bears.dataloaders(path)\ndls.valid.show_batch(max_n=4, nrows=1)","metadata":{"_uuid":"03679d65-eb5b-492e-bb95-2e5cf64bdced","_cell_guid":"b3fa94e1-5376-456a-a67d-f75d7bf0f378","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Understanding `ResizeMethod.pad`\n\n- **`ResizeMethod.pad`**:\n  - A resizing method in FastAI used to resize images to a specific size while maintaining the aspect ratio.\n  - Adds padding (empty space) to the shorter side of the image to ensure the final dimensions match the desired size.\n\n#### Key Points:\n- **Aspect Ratio**: Preserves the original aspect ratio of the image.\n- **Padding**:\n  - The shorter side of the image is padded with a specified color (default is black) to match the desired dimensions.\n  - This avoids distortion or cropping of the image.\n- **Use Case**:\n  - Ideal when you want to maintain the entire content of an image without altering its proportions.","metadata":{"_uuid":"a6702fa2-be05-4232-9d3c-dbbf4c7291ee","_cell_guid":"6855fbec-a965-4a1c-8445-b7aeb52853d7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"bears = bears.new(item_tfms=Resize(128, ResizeMethod.Pad, pad_mode='zeros'))\ndls = bears.dataloaders(path)\ndls.valid.show_batch(max_n=4, nrows=1)","metadata":{"_uuid":"bf14d2e4-b67d-4c3f-a80e-f711ffffa6c3","_cell_guid":"f6555e05-3aee-46bd-a7e6-bc69e3e31142","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Understanding `RandomResizedCrop`\n\n- **Purpose**:\n  - `RandomResizedCrop` is a data augmentation technique commonly used in image processing pipelines.\n  - It randomly crops a region from an image and resizes it to a specified size. This helps introduce variability in the dataset and improves model generalization.\n\n---\n\n### Parameters\n\n- **`size`**:\n  - The target size (e.g., `128` or `(128, 128)`) to which the cropped region will be resized.\n- **`min_scale`**:\n  - The minimum scale for the cropped region as a fraction of the original image size (default: `0.08`).\n  - Larger values result in smaller crop regions.\n- **`ratio`**:\n  - Aspect ratio range for the cropped region (default: `(3/4, 4/3)`).\n  - Ensures the crop doesn’t distort the image too much.\n- **`resamples`**:\n  - Resampling filters used during resizing (e.g., `Image.BILINEAR`).\n\n---\n\n### Key Benefits\n\n- Introduces randomness to the dataset, preventing the model from overfitting to specific image details.\n- Ensures all images have the same size after augmentation, which is required for neural network input.\n- Provides robustness to variations in object position, size, and aspect ratio.","metadata":{"_uuid":"d07c0f3e-2fcf-461a-889f-9802e24e3c14","_cell_guid":"deddfe7c-13ca-4d3e-8be4-67d6163c17b2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from fastai.vision.all import RandomResizedCrop\n\nbears = bears.new(item_tfms=RandomResizedCrop(128, min_scale=0.3))\ndls = bears.dataloaders(path)\ndls.train.show_batch(max_n=4, nrows=1, unique=True)","metadata":{"_uuid":"bb246524-b067-4e1a-bba5-1a668d433bb5","_cell_guid":"635b44f9-901d-417c-8ab0-a2e449d74eda","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Augmentation\n***Data augmentation*** - creating random variations of our input data\n- Examples of common data augmentation techniques for images (rotation, flipping, perspective warping, brightness and contrast changes)\n- To tell fastai we want to use these transforms on a batch, we use the **batch_tfms** parameter\n- We're also using double the amount of augmentation compared to the default","metadata":{"_uuid":"cd74d64c-1060-4613-bcee-7542d118f896","_cell_guid":"763abcef-c99f-42c5-932a-59a7eb0af881","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from fastai.vision.augment import aug_transforms\nbears = bears.new(item_tfms=Resize(128), batch_tfms=aug_transforms(mult=2))\ndls = bears.dataloaders(path)\ndls.train.show_batch(max_n=8, nrows=2, unique=True)","metadata":{"_uuid":"c0ff7620-e740-46ea-93f6-fcabe9d9c9eb","_cell_guid":"d5c7bf3c-5cbe-45bb-aae1-2df3541979b6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bears = bears.new(\n    #RandomResizedCrop with an image size of 224 px, which is fairly standard for image classification\n    item_tfms=RandomResizedCrop(224, min_scale=0.5), \n    batch_tfms=aug_transforms())\ndls = bears.dataloaders(path)","metadata":{"_uuid":"b6d270f6-72ff-4506-b05d-bac6f5e5a125","_cell_guid":"847a79a9-9b51-4590-9255-f762a4c4fc69","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train a deep learning model for image classifictaion using the FastAI library\n\nThis code is responsible for validating a collection of image files and removing any invalid or unreadable images. Here's a step-by-step explanation:\n\n\n### Step 1: Create a Vision Learner\n**What it does:**\n- `vision_learner`\n  - A helper function in FastAI that simplifies creating a deep learning model for image-related\n  - Automatically integrates the data (`dls`), architecture (`resnet18`), and evaluation metric (`error_rate`).\n- **Parameters**:\n    - `dls`: The `DataLoaders` object containing the training and validation datasets.\n    - `resnet18`: A pre-trained convolutional neural network (CNN) model (ResNet-18).\n        - ResNet-18 is a lightweight architecture commonly used for image classification tasks.\n        - Pre-trained weights (on ImageNet) are used, which speeds up training and improves accuracy.\n    - `metrics=error_rate`:\n        - Specifies the evaluation metric.\n        - `error_rate`: The fraction of incorrect predictions (1 - accuracy).\n- **Output**:\n    - Returns a `Learner` object (`learn`), which encapsulates the model, data, and training configurations.\n \n> A Learner object in FastAI serves as the central interface for training, validating, and interpreting machine learning models. It simplifies the entire training process by seamlessly connecting the dataset, model, and evaluation metrics into a cohesive framework.\n\n---\n\n### Step 2: Fine-Tune the Model\n**What it does:**:\n- `fine_tune`\n  - Adapts the pre-trained ResNet-18 model to the new dataset using transfer learning\n  - Fine-tuning adjusts the model's parameters to perform well on the current task while leveraging the pre-trained weights.\n- **Parameter**:\n    - `4`: The number of epochs for fine-tuning.\n        - One epoch means the model processes the entire dataset once during training.\n        - In this case, the model fine-tunes for 4 epochs.\n- **Steps in Fine-Tuning**:\n    - **Stage 1**: Trains only the final (head) layer of the model (newly added for the dataset) while freezing the pre-trained layers.\n    - **Stage 2**: Unfreezes the entire model and fine-tunes all layers with a smaller learning rate.\n\n ---\n  \n### Purpose and Benefits\n\n- **Transfer Learning**: \n  - The pre-trained ResNet-18 model already knows how to extract features from images (e.g., edges, textures, shapes).\n  - Fine-tuning adjusts these features to the specific dataset, saving time and improving performance.\n\n- **Error Rate**:\n  - Tracks the fraction of incorrect predictions during training, helping monitor model performance.","metadata":{"_uuid":"a35edf03-7df6-4984-b7b6-75d2ef5fa420","_cell_guid":"1838acd6-bd19-4115-bbed-141679b97a23","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from fastai.vision.all import vision_learner, resnet18, error_rate\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(4)","metadata":{"_uuid":"cfdc2038-5716-4975-aba8-4d0c401dd9d8","_cell_guid":"8df57fe5-9fdb-46c9-935c-1a908f44468d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Sample Results and Explanation\n\nThe results displayed in this table are typical outputs from training a machine learning model using FastAI, showing metrics for each epoch during training and fine-tuning. Here’s a detailed explanation of each column and what these results indicate:\n\n---\n\n### Columns in the Output\n- `epoch`:\n    - Represents the current epoch (one full pass through the training dataset).\n    - Starts at 0 and increments with each additional epoch.\n- `train_loss`:\n    - The loss calculated on the training dataset for that epoch\n    - A lower value indicates the modle is performing better on the training data\n- `valid_loss`:\n    - The loss calculated on the validation dataset for that epoch\n    - A decreasing validation loss indicates the model is generalizing well to unseen data\n- `error_rate`:\n    - A metric representing the fraction of incorrect predictions (1 - accuracy).\n    - A lower value is better and indicates that the model is making fewer incorrect predictions.\n- `time`:\n    - The time taken to complete the epoch (training and validation combined).\n \n---\n\n### Results Interpretation\n**First Table (Before Fine-Tuning)**\n- The model trains for one epoch and achieves the following:\n    - `train_loss`:1.074360 — High at the start, as expected for an untrained model\n    - `valid_loss`:0.098378 — Significantly lower than train_loss, suggesting that the model quickly learns from the data.\n    - `error_rate`:0.033333 — Indicates that the model has an error rate of 3.3% on the validation dataset after one epoch.\n      \nThis stage likely corresponds to training only the \"head\" of the pre-trained model (the final classification layer), while the other layers remain frozen.  \n\n**Second Table**\n- The model undergoes 4 epochs of fine-tuning, where all layers are unfrozen, and the entire network is trained\n    - **Epoch 0**:\n        - `train_loss` drops significantly to 0.142627, showing improvement as more layers are trained.\n        - `valid_loss` is 0.081108, indicating good performance on the validation set.\n        - `error_rate` is 0.026667 (2.7%), showing better accuracy than the first stage.\n    - **Epoch 1-3**:\n        - Both `train_loss` and `valid_loss` continue to decrease\n            - `train_loss` reaches 0.050202 at the end, suggesting that the model fits the training data well\n            - `valid_loss` stabilizes at 0.047072, showing good generalization to the validation set.\n        - The `error_rate` drops further to 0.013333 (1.3%), reflecting an improved ability to classify validation data correctly.\n\n---\n\n### Key Observations\n- **Decreasing Loss and Error Rate**\n    - Both `train_loss` and `valid_loss` decrease over time, which is expected during successful training and fine-tuning.\n    - The `error_rate` also decreases, confirming improved model accuracy.\n- **Consistency in Validation Performance**\n    - The gap between `train_loss` and `valid_loss` remains small, which suggests the model is not overfitting and generalizes well to unseen data.\n- **Effectiveness of Fine-Tuning**\n    - Fine-tuning (unfreezing and training all layers) significantly improves both the `valid_loss` and `error_rate` compared to the initial stage.","metadata":{"_uuid":"9343b073-f201-40e1-a340-132bc0811bf3","_cell_guid":"601cef98-19d8-4f31-87e6-8f27d6c4d742","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"# What is `ClassificationInterpretation`?\n\nThe `ClassificationInterpretation` class in FastAI provides tools to evaluate the performance of a trained classification model. It helps analyze where the model performs well and where it struggles, offering insights into its predictions.\n\n---\n\n## Key Features of `ClassificationInterpretation`\n\n### 1. Confusion Matrix\n- Visualizes the relationship between predicted and actual classes.\n- Helps identify patterns in misclassifications.\n\n### 2. Top Losses\n- Identifies and displays the samples where the model had the highest prediction loss.\n- Useful for diagnosing why the model struggled with specific examples.\n\n### 3. Most Confused Classes\n- Lists the pairs of classes the model most frequently confuses.\n- Helps understand where the model’s predictions overlap.","metadata":{"_uuid":"1f64f229-2ac9-4db4-8139-d664dd7199c6","_cell_guid":"ca503aa7-6cdf-47d7-9067-263d546d9e99","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from fastai.vision.all import ClassificationInterpretation\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()","metadata":{"_uuid":"de2668bd-72bf-47c0-80ab-026a534ab8dd","_cell_guid":"90673a98-761e-4013-ab15-82d20b39a3d0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Understanding the Confusion Matrix and Loss in FastAI\n\n### Confusion Matrix\n- **Rows**: Represent the actual classes in the dataset (e.g., black, grizzly, and teddy bears).\n- **Columns**: Represent the predicted classes (e.g., black, grizzly, and teddy bears).\n- **Diagonal**: Shows the correctly classified images.\n- **Off-diagonal cells**: Show misclassified images.\n- The goal is to have **dark blue on the diagonal** (correct predictions) and **white elsewhere** (few mistakes).\n- The confusion matrix is calculated using the validation set, and FastAI makes it easy to visualize model results.\n\n### Analyzing Errors\n- Understanding errors helps identify whether they're caused by:\n  - **Dataset problems** (e.g., mislabeled data or non-bear images).\n  - **Model limitations** (e.g., struggles with unusual lighting or angles).\n\n### Loss\n- **Definition**: A number representing how well the model’s prediction matches the actual label.\n  - Higher loss: Model is wrong (especially if it’s confident about the wrong answer).\n  - Lower loss: Model is correct or unsure about its prediction.\n- **`plot_top_losses`**: Displays the images with the highest loss, helping identify challenging cases.\n- Each image is labeled with:\n  1. **Prediction**: The model’s predicted class.\n  2. **Actual**: The correct label.\n  3. **Loss**: The calculated loss for that prediction.\n  4. **Probability**: The model’s confidence level (from 0 to 1) in its prediction.","metadata":{"_uuid":"e252b340-503a-4cac-9c14-f89f3c5c1f42","_cell_guid":"044695a5-fa17-4634-99c7-55d520262923","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"interp.plot_top_losses(5, nrows=1)","metadata":{"_uuid":"061f2131-04a3-4f64-ab31-22231da1fe17","_cell_guid":"67293ff1-cbb0-4940-8fd1-5bfa9eba7896","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* The intuitive approach to doing data cleaning is to do it before you train a model. But a model can actually help you find data issues more quickly and easily. So, we normally prefer to train a quick and simple model first, and then use it to help us with data cleaning.\n* fastai includes a handy GUI for data cleaning called **ImageClassifierCleaner** that allows you to choose a category and the training versus validation set and view the highest-loss images (in order), along with menus to allow images to be selected for removal or relabeling","metadata":{"_uuid":"cd984161-7c35-4962-ad93-073aa4edb812","_cell_guid":"4c55e29c-d06f-4b15-9739-ff687da3efdf","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from fastai.vision.widgets import ImageClassifierCleaner\nfrom ipywidgets import VBox \n# IPython widgets are GUI components that bring together JavaScript and Python functionality in a web browser\n\ncleaner = ImageClassifierCleaner(learn)\ncleaner","metadata":{"_uuid":"46a209a6-280d-4666-affe-9b9ad73d0c57","_cell_guid":"4c419408-d627-4917-a6d3-3f70c3f96b04","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"execution_failed":"2025-01-20T20:47:35.172Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# to delete (unlink) all images selected for deletion, we would run:\nfor idx in cleaner.delete(): cleaner.fns[idx].unlink()\n    \n# To move images for which we've selected a different category, we would run:\nfor idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)","metadata":{"_uuid":"6ad242f7-dc2b-4fda-9466-6cfcdf3a5206","_cell_guid":"f5067d8c-3161-4072-9d70-35b1434323c7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"execution_failed":"2025-01-20T20:47:35.172Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Using the Model for Inference**\n\n***When we use a model for getting predictions, instead of training, we call it inference***\n\nOnce you've trained and finalized a model, the next step is to save it so that it can be used in production. This involves copying the saved model to a production server or environment for inference. Here's how you can efficiently save and prepare your model:\n\n---\n\n#### **Why Save the Model?**\n- Saving ensures you retain both:\n  1. **The architecture**: The structure of the neural network.\n  2. **The trained parameters**: The learned weights and biases from training.\n  \n  Combining these ensures that when you load the model, the architecture and parameters are correctly aligned, avoiding mismatches or errors.\n\n---\n\n#### **How to Save the Model?**\n- Use the `export` method in FastAI:\n  ```python\n  learn.export()","metadata":{"_uuid":"f296dfbd-ee5a-41ea-8890-f975ca801a4b","_cell_guid":"91ccc2ab-6775-4df8-a545-5422dd14b047","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"learn.export()\n\npath = Path()\npath.ls(file_exts='.pkl')","metadata":{"_uuid":"3479624e-6736-4047-9ea5-868364c43275","_cell_guid":"3851f7d8-f0d1-4339-b2b5-9e26ae2943af","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"execution_failed":"2025-01-20T20:47:35.174Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"When we use a model for getting predictions, instead of training, we call it inference. To create our inference learner from the exported file, we use load_learner (in this case, this isn't really necessary, since we already have a working Learner in our notebook; we're just doing it here so you can see the whole process end-to-end)\n\nWhen we're doing inference, we're generally just getting predictions for one image at a time. To do this, pass a filename to predict:","metadata":{"_uuid":"190e43d5-2e4b-440e-85f1-46aede999d7d","_cell_guid":"e8035701-88c5-448e-8acc-78cdc944f0da","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"learn_inf = load_learner(path/'export.pkl')\nlearn_inf.predict('images/grizzly.jpg')","metadata":{"_uuid":"d031db36-8894-40b6-bf4f-fba4227ea121","_cell_guid":"c2ebdb3e-babe-4e76-b1cf-c517305aa386","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Understanding the Prediction Output**\n\nWhen analyzing the output of `learn_inf.predict`, it provides detailed information about the model's prediction. Here's a breakdown:\n\n---\n\n#### **1. Predicted Class**\n- The first element in the output, `'grizzly'`, represents the **predicted class** for the input image.\n- This is a string label corresponding to the category that the model predicts the input image belongs to.\n- In this case, the model predicts that the image belongs to the `'grizzly'` class.\n\n---\n\n#### **2. Tensor Index**\n- The second element, `tensor(1)`, is a **numerical representation of the predicted class index**.\n- FastAI assigns an integer index to each class in your dataset. For example:\n  - `0`: Black bear\n  - `1`: Grizzly bear\n  - `2`: Teddy bear\n- In this case, the tensor value `1` corresponds to the `'grizzly'` class.\n\n---\n\n#### **3. Prediction Probabilities**\n- The third element, `tensor([5.4334e-06, 9.9999e-01, 3.2537e-06])`, is a **tensor of probabilities** for each class.\n- This shows the model's confidence in its prediction for each class.\n- Each value represents the likelihood of the input image belonging to a specific class, in the same order as the class indices:\n  - `0`: Black bear → `5.4334e-06` (~0.000005%)\n  - `1`: Grizzly bear → `9.9999e-01` (~99.9999%)\n  - `2`: Teddy bear → `3.2537e-06` (~0.000003%)\n- In this case, the model is almost certain (with a 99.9999% probability) that the input image is a `'grizzly'`.\n\n---\n\n#### **Summary**\nThe output indicates:\n1. The model predicts the input image is of class `'grizzly'`.\n2. The numerical index of this class is `1`.\n3. The model is highly confident in this prediction, assigning nearly 100% probability to the `'grizzly'` class.\n\nThis comprehensive output provides both the predicted label and detailed probabilities, which can help evaluate the confidence and reliability of the model’s prediction.","metadata":{"_uuid":"6902a281-875d-401e-b446-9b2e61216ca8","_cell_guid":"38f230e9-961a-474c-96fa-630666fc7adb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nuser_credential = user_secrets.get_gcloud_credential()\nuser_secrets.set_tensorflow_credential(user_credential)","metadata":{"_uuid":"547b5c68-5235-43f7-b9ef-70ce86bdd347","_cell_guid":"d44a506c-1a15-41ac-a157-d295784f4aa4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"learn_inf.dls.vocab","metadata":{"_uuid":"9b0a8d6c-6ad6-4a76-9f6d-f70e7d5abbee","_cell_guid":"17e68f38-b018-45e7-bb0d-aeff76fb19fc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"learn_inf = load_learner(path/'export.pkl')\nlearn_inf.predict('/kaggle/input/bearimage2/bear2.jpg')","metadata":{"_uuid":"bb50fc73-d5fa-42da-b6da-e8693ea5572a","_cell_guid":"029329b2-b465-45ae-91a4-72d1c88f8e44","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"learn_inf.dls.vocab","metadata":{"_uuid":"ac5658a1-4003-40e3-bc37-6ca524313651","_cell_guid":"28b0b589-5f55-4d6f-9bd7-2e6dfbc4a3f5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Creating a Notbook Web Application from the Model\n\n`Voilà` is a system for making applications consisting of IPython widgets available to end users, without them having to use Jupyter at all.","metadata":{"_uuid":"ccf8928c-a660-4eca-986b-178aeba54d6c","_cell_guid":"dc8fdf3d-8e5d-4c24-990a-066b63a4547c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from ipywidgets import widgets, VBox\nfrom IPython.display import display\nfrom fastai.vision.all import PILImage\nimport os\n\n# File upload widget\nbtn_upload = widgets.FileUpload(accept='image/*', multiple=False)\nlbl_pred = widgets.Label()\nbtn_run = widgets.Button(description='Classify')\nout_pl = widgets.Output()\n\n# Function to handle classification\ndef on_click_classify(change):\n    out_pl.clear_output()  # Clear previous output\n    if btn_upload.value:\n        print(\"Raw btn_upload.value:\", btn_upload.value)  # Debugging log\n        \n        # Access the first file in the tuple\n        uploaded_file = btn_upload.value[0]  # Access tuple directly\n        print(f\"Uploaded file details: {uploaded_file}\")\n\n        # Extract file content and metadata\n        try:\n            file_content = uploaded_file['content']\n            filename = uploaded_file['name']\n            print(f\"File name: {filename}\")\n            \n            # Save the file to a local path\n            temp_path = f'/kaggle/working/{filename}'\n            with open(temp_path, 'wb') as f:\n                f.write(file_content.tobytes())  # Convert memoryview to bytes\n            \n            print(f\"File saved to: {temp_path}\")\n            \n            # Try to load and display the image\n            try:\n                img = PILImage.create(temp_path)\n                print(\"Image successfully loaded.\")\n                with out_pl:\n                    display(img.to_thumb(128, 128))\n\n                # Perform prediction\n                pred, pred_idx, probs = learn_inf.predict(img)\n                lbl_pred.value = f'Prediction: {pred}; Probability: {probs[pred_idx]:.04f}'\n            except Exception as e:\n                print(\"Error loading image with PILImage.create:\", e)\n                lbl_pred.value = \"Failed to process the uploaded image. Ensure it's a valid image file.\"\n        except Exception as e:\n            print(\"Error handling file content:\", e)\n            lbl_pred.value = \"Error processing the uploaded file. Please try again.\"\n    else:\n        lbl_pred.value = 'Please upload an image first!'\n\n# Attach event to button\nbtn_run.on_click(on_click_classify)\n\n# Display widgets in a vertical layout\ndisplay(VBox([\n    widgets.Label('Select your bear!'),\n    btn_upload,\n    btn_run,\n    out_pl,\n    lbl_pred\n]))","metadata":{"_uuid":"8899653c-33a2-497a-98ff-f6e62e24c150","_cell_guid":"3c69a353-f5ba-4d59-88c7-624bda897806","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}